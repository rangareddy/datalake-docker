FROM python:3.10-bullseye

SHELL ["/bin/bash", "-c"]

LABEL maintainer="Ranga Reddy <rangareddy.avula@gmail.com>"
LABEL description="Datalake Spark image"

RUN set -ex; \
  pip install --upgrade pip && \
  pip install faker && \
  apt-get update && \
  apt-get install -y --no-install-recommends \
  sudo curl telnet wget tar vim ssh \
  vim unzip rsync openjdk-11-jdk \
  build-essential software-properties-common && \
  apt-get clean && \
  rm -rf /var/lib/apt/lists/*

ARG SPARK_VERSION=${SPARK_VERSION:-3.5.3}
ARG SCALA_VERSION=${SCALA_VERSION:-2.12}
ARG SPARK_HOME=${SPARK_HOME:-/opt/spark}
ARG HUDI_TARGET_DIR=$HUDI_TARGET_DIR
ARG ICEBERG_VERSION=${ICEBERG_VERSION:-1.7.1}
ARG DELTA_VERSION=${DELTA_VERSION:-3.3.0}

ENV SPARK_VERSION=${SPARK_VERSION:-3.5.3} \
  SPARK_MAJOR_VERSION=${SPARK_MAJOR_VERSION:-3.5} \
  SPARK_HOME=$SPARK_HOME \
  SPARK_CONF=$SPARK_HOME/conf \
  SPARK_CONF_DIR=/etc/spark/conf \
  SPARK_LOG_DIR=${SPARK_LOG_DIR:-/var/log/spark} \
  HUDI_HOME=${HUDI_HOME:-/opt/hudi} \
  PATH="${SPARK_HOME}/sbin:${SPARK_HOME}/bin:${PATH}"

COPY software/spark-${SPARK_VERSION}-bin-hadoop3.tgz /tmp/spark-${SPARK_VERSION}-bin-hadoop3.tgz

RUN mkdir -p ${SPARK_HOME} && \
  mkdir -p $HUDI_HOME && \
  mkdir -p ${SPARK_HOME}/spark-events && \
  mkdir -p $SPARK_LOG_DIR && \
  tar xzf /tmp/spark-${SPARK_VERSION}-bin-hadoop3.tgz --directory ${SPARK_HOME} --strip-components 1 && \
  rm -rf /tmp/spark-${SPARK_VERSION}-bin-hadoop3.tgz

COPY hadoop-s3-jars/* $SPARK_HOME/jars/
COPY db_connector_jars/* $SPARK_HOME/jars/
COPY $HUDI_TARGET_DIR/hudi-spark-bundle $HUDI_HOME/hudi-spark-bundle
COPY $HUDI_TARGET_DIR/hudi-utilities-slim-bundle $HUDI_HOME/hudi-utilities-slim-bundle
COPY $HUDI_TARGET_DIR/hudi-hive-sync-bundle $HUDI_HOME/hudi-hive-sync-bundle
COPY $HUDI_TARGET_DIR/hudi-cli-bundle $HUDI_HOME/hudi-cli-bundle
COPY conf/spark/* $SPARK_CONF/
COPY conf/hadoop/core-site.xml $SPARK_CONF/
RUN mkdir -p /etc/spark && ln -sf $SPARK_CONF $SPARK_CONF_DIR

COPY requirements.txt /tmp/requirements.txt
RUN pip install --no-cache-dir -r /tmp/requirements.txt
RUN python3 -m spylon_kernel install

# Download and install IJava jupyter kernel
RUN curl -s https://github.com/SpencerPark/IJava/releases/download/v1.3.0/ijava-1.3.0.zip -Lo ijava-1.3.0.zip \
  && unzip ijava-1.3.0.zip \
  && python3 install.py --sys-prefix \
  && rm ijava-1.3.0.zip

ARG ICEBERG_JAR=iceberg-spark-runtime-${SPARK_MAJOR_VERSION}_${SCALA_VERSION}-${ICEBERG_VERSION}.jar
ARG AVRO_JAR=spark-avro_${SCALA_VERSION}-${SPARK_VERSION}.jar
ARG DELTA_JAR=delta-spark_${SCALA_VERSION}-${DELTA_VERSION}.jar

RUN echo "export CLI_BUNDLE_JAR=$(ls "$HUDI_HOME"/hudi-cli-bundle/hudi-cli-bundle*.jar)" >> /root/.bashrc && \
  echo "export SPARK_BUNDLE_JAR=$(ls "$HUDI_HOME"/hudi-spark-bundle/hudi-spark*-bundle*.jar)" >> /root/.bashrc 

ARG APACHE_URL="https://repo1.maven.org/maven2/org/apache"

RUN wget -P $SPARK_HOME/jars/ $APACHE_URL/spark/spark-avro_${SCALA_VERSION}/${SPARK_VERSION}/$AVRO_JAR && \
  wget -P $SPARK_HOME/jars/ $APACHE_URL/iceberg/iceberg-spark-runtime-${SPARK_MAJOR_VERSION}_${SCALA_VERSION}/$ICEBERG_VERSION/$ICEBERG_JAR && \
  wget -P $SPARK_HOME/jars/ https://repo1.maven.org/maven2/io/delta/delta-spark_${SCALA_VERSION}/${DELTA_VERSION}/$DELTA_JAR && \
  ln -sf $HUDI_HOME/hudi-spark-bundle/hudi-spark${SPARK_MAJOR_VERSION}-bundle_${SCALA_VERSION}*.jar $SPARK_HOME/jars/

COPY scripts/spark/entrypoint.sh /opt/entrypoint.sh
COPY check_service_status_utility.sh /opt/check_service_status_utility.sh

RUN chmod 755 /opt/entrypoint.sh /opt/check_service_status_utility.sh
WORKDIR $SPARK_HOME

ENTRYPOINT ["/opt/entrypoint.sh"]